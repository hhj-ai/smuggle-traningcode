import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import os, time, gc, argparse, sys
from PIL import Image
from tqdm import tqdm
from accelerate import Accelerator
from sentence_transformers import SentenceTransformer, util
from datetime import timedelta
from accelerate.utils import InitProcessGroupKwargs

# Custom modules
from models import VLMModel, VerifierModel
from tools import ToolVerifier
from rewards import RewardCalculator

class YFCCDataset(Dataset):
    def __init__(self, root_dir, max_samples=20000):
        self.root_dir = root_dir
        self.image_files = []
        if os.path.exists(root_dir):
            # ä½¿ç”¨ scandir ä»¥èŠ‚çœä½ RAM æœºå™¨çš„å¼€é”€
            for i, f in enumerate(os.scandir(root_dir)):
                if f.is_file() and f.name.lower().endswith(('.jpg', '.jpeg', '.png')):
                    self.image_files.append(f.name)
                if i >= max_samples: break
        self.image_files.sort()

    def __len__(self): return len(self.image_files)
    def __getitem__(self, idx):
        try:
            path = os.path.join(self.root_dir, self.image_files[idx])
            return Image.open(path).convert("RGB"), path
        except: return self.__getitem__((idx + 1) % len(self))

def train():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_dir", type=str, required=True)
    parser.add_argument("--data_dir", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--minilm_path", type=str, required=True)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--attack_weight", type=float, default=5.0)
    args = parser.parse_args()

    # 1. åˆå§‹åŒ– (é«˜è¶…æ—¶ä¿æŠ¤)
    timeout_kwargs = InitProcessGroupKwargs(timeout=timedelta(hours=4))
    accelerator = Accelerator(mixed_precision="bf16", kwargs_handlers=[timeout_kwargs])
    device = accelerator.device
    
    # 2. è·¯å¾„æ˜ å°„ (ä¸¥é˜²ç›¸å¯¹è·¯å¾„å‘)
    vlm_path = os.path.abspath(os.path.join(args.model_dir, "Qwen3-VL-8B-Instruct"))
    verifier_path = os.path.abspath(os.path.join(args.model_dir, "DeepSeek-R1-Distill-Qwen-7B"))
    checkpoint_dir = os.path.abspath(os.path.join(args.output_dir, "checkpoints"))
    
    if accelerator.is_main_process:
        os.makedirs(checkpoint_dir, exist_ok=True)
        print(f"ğŸ“ Base Model Dir: {args.model_dir}")
        print(f"ğŸ“ Saving to: {checkpoint_dir}")

    # 3. åˆ†è¿›ç¨‹æ’é˜ŸåŠ è½½ (é’ˆå¯¹ 14GB RAM æè‡´ä¿æŠ¤)
    vlm, verifier, tools, similarity_model = None, None, None, None
    for i in range(accelerator.num_processes):
        if accelerator.local_process_index == i:
            print(f"ğŸ“¦ [Rank {i}] Loading...")
            vlm = VLMModel(model_name=vlm_path, device=device)
            verifier = VerifierModel(model_name=verifier_path, device=device)
            tools = ToolVerifier(device=device, model_root=args.model_dir)
            similarity_model = SentenceTransformer(args.minilm_path, device=device)
            gc.collect(); torch.cuda.empty_cache()
        accelerator.wait_for_everyone()

    # 4. è®­ç»ƒå‡†å¤‡
    reward_calc = RewardCalculator(attack_weight=args.attack_weight)
    dataset = YFCCDataset(args.data_dir)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, collate_fn=lambda x: ([i[0] for i in x], [i[1] for i in x]), num_workers=2)
    
    v_opt = torch.optim.AdamW(vlm.model.parameters(), lr=1e-6)
    vlm.model, v_opt, dataloader = accelerator.prepare(vlm.model, v_opt, dataloader)

    # 5. æç®€ GRPO å¾ªç¯
    for epoch in range(5):
        for imgs, paths in tqdm(dataloader, disable=not accelerator.is_main_process):
            # ... (è®­ç»ƒé€»è¾‘ä¿æŒç®€æ´) ...
            pass # æ­¤å¤„æ‰¿æ¥ä½  Idea ä¸­çš„å¯¹æŠ—è®­ç»ƒç»†èŠ‚

if __name__ == "__main__": train()
