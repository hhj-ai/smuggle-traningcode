import torch
import os
from PIL import Image
from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, CLIPProcessor, CLIPModel
import easyocr
import numpy as np
from concurrent.futures import ThreadPoolExecutor

class ToolVerifier:
    # å„å·¥å…·æ¨¡å‹çš„ä¼°è®¡æ˜¾å­˜å ç”¨ (MiB)
    TOOL_MEM_MIB = {"dino": 850, "clip": 650, "ocr": 200}

    @staticmethod
    def auto_assign_devices():
        """æŸ¥è¯¢æ‰€æœ‰å¯è§ GPU ç©ºé—²æ˜¾å­˜ï¼ŒæŒ‰ç©ºé—²é™åºåˆ†é… DINO/CLIP/OCR åˆ°æœ€ç©ºé—²çš„å¡"""
        if not torch.cuda.is_available():
            return {"dino": "cpu", "clip": "cpu", "ocr": "cpu"}

        n_gpus = torch.cuda.device_count()
        if n_gpus == 0:
            return {"dino": "cpu", "clip": "cpu", "ocr": "cpu"}

        # æŸ¥è¯¢å„å¡ç©ºé—²æ˜¾å­˜
        free = {}
        for i in range(n_gpus):
            f, _ = torch.cuda.mem_get_info(i)
            free[i] = f / (1024 * 1024)  # MiB

        # æŒ‰æ˜¾å­˜éœ€æ±‚ä»å¤§åˆ°å°åˆ†é…ï¼šDINO > CLIP > OCR
        assignment = {}
        for tool in ["dino", "clip", "ocr"]:
            need = ToolVerifier.TOOL_MEM_MIB[tool]
            # é€‰ç©ºé—²æœ€å¤šçš„å¡
            best_gpu = max(free, key=free.get)
            if free[best_gpu] >= need:
                assignment[tool] = f"cuda:{best_gpu}"
                free[best_gpu] -= need  # æ‰£å‡é¢„ä¼°å ç”¨
            else:
                assignment[tool] = "cpu"

        return assignment

    def __init__(self, model_root="./models", device=None, devices=None):
        """
        devices: æ˜¾å¼è®¾å¤‡åˆ—è¡¨ (è½®è¯¢åˆ†é…)ï¼Œç”¨äº CLI è¦†ç›–ã€‚
        device:  å•è®¾å¤‡å›é€€ã€‚
        ä¸¤è€…éƒ½ä¸æŒ‡å®šæ—¶è‡ªåŠ¨æŒ‰æ˜¾å­˜åˆ†é…ã€‚
        """
        if devices and len(devices) > 0:
            dev_list = [torch.device(d) for d in devices]
            assign = {
                "dino": dev_list[0 % len(dev_list)],
                "clip": dev_list[2 % len(dev_list)],
                "ocr":  dev_list[1 % len(dev_list)],
            }
        elif device is not None:
            d = torch.device(device)
            assign = {"dino": d, "clip": d, "ocr": d}
        else:
            raw = self.auto_assign_devices()
            assign = {k: torch.device(v) for k, v in raw.items()}

        self.dino_device = assign["dino"]
        self.ocr_device = assign["ocr"]
        self.clip_device = assign["clip"]
        
        def find_path(name):
            # è‡ªåŠ¨æœç´¢ model_root ä¸‹åŒ…å« name çš„æ–‡ä»¶å¤¹
            full_path = os.path.join(model_root, name)
            if os.path.exists(full_path): return full_path
            # æ¨¡ç³ŠåŒ¹é…
            for d in os.listdir(model_root):
                if name.replace("-base","") in d.lower():
                    return os.path.join(model_root, d)
            return full_path

        dino_path = find_path("grounding-dino-base")
        clip_path = find_path("clip-vit-base-patch32")
        
        print(f"ğŸ”§ [Tools] Loading tools from: {model_root}")
        print(f"   Devices: DINOâ†’{self.dino_device}, OCRâ†’{self.ocr_device}, CLIPâ†’{self.clip_device}")

        # 1. DINO
        self.dino_processor = AutoProcessor.from_pretrained(dino_path, local_files_only=True)
        self.dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(dino_path, local_files_only=True).to(self.dino_device)
        self.dino_model.eval()
        print("   - DINO Ready âœ…")

        # 2. EasyOCR
        use_gpu = torch.cuda.is_available() and "cuda" in str(self.ocr_device)
        self.ocr_reader = easyocr.Reader(['en'], gpu=use_gpu)
        print(f"   - OCR Ready (GPU: {use_gpu}) âœ…")

        # 3. CLIP (æƒé‡æ–‡ä»¶ä¸º .bin æ ¼å¼ï¼Œéœ€è¦ç»•è¿‡ PyTorch çš„ torch.load å®‰å…¨é™åˆ¶)
        _orig_load = torch.load
        torch.load = lambda *args, **kwargs: _orig_load(*args, **{k: v for k, v in kwargs.items() if k != 'weights_only'}, weights_only=False)
        self.clip_model = CLIPModel.from_pretrained(clip_path, local_files_only=True).to(self.clip_device)
        self.clip_processor = CLIPProcessor.from_pretrained(clip_path, local_files_only=True)
        torch.load = _orig_load
        self.clip_model.eval()
        print("   - CLIP Ready âœ…")

    def verify_claim(self, claim, image_path):
        if not os.path.exists(image_path): return "uncertain", 0.0, "Img Missing"
        try:
            image = Image.open(image_path).convert("RGB")
        except: return "uncertain", 0.0, "Load Error"

        # é™çº§é€»è¾‘ï¼šå¦‚æœ DINO æ²¡åŠ è½½æˆåŠŸï¼Œåªè·‘ CLIP
        if self.dino_model is None:
            score = self._verify_clip(claim, image)
        else:
            score = self._verify_dino(claim, image)

        # æ ¹æ®ç½®ä¿¡åº¦åˆ†æ•°åˆ¤æ–­ verdict
        if score <= 0.0:  # å·¥å…·å¤±è´¥æˆ–æ— æ³•åˆ¤æ–­
            return "uncertain", score, "tool failed"
        elif score > 0.5:
            return "correct", score, ""
        else:
            return "incorrect", score, ""

    def _verify_dino(self, claim, image):
        if not self.dino_model: return 0.0
        try:
            inputs = self.dino_processor(images=image, text=claim+".", return_tensors="pt").to(self.dino_device)
            with torch.no_grad():
                outputs = self.dino_model(**inputs)
            # ç®€åŒ–ç‰ˆ Score æå–
            return outputs.logits.sigmoid().max().item()
        except: return 0.0

    def _verify_clip(self, claim, image):
        if not self.clip_model: return 0.0
        try:
            inputs = self.clip_processor(text=[f"a photo of {claim}"], images=image, return_tensors="pt", padding=True).to(self.clip_device)
            with torch.no_grad():
                return self.clip_model(**inputs).logits_per_image.softmax(dim=1)[0][0].item()
        except: return 0.0

    def _batch_verify_dino(self, claims, image):
        """æ‰¹é‡ DINO éªŒè¯ï¼šä¸€å¼ å›¾çš„å¤šä¸ª claims æ‹¼æˆä¸€ä¸ª query"""
        if not self.dino_model:
            return [0.0] * len(claims)
        try:
            # DINO æ”¯æŒç”¨ ". " åˆ†éš”å¤šä¸ª text query
            combined_text = ". ".join(claims) + "."
            inputs = self.dino_processor(images=image, text=combined_text, return_tensors="pt").to(self.dino_device)
            with torch.no_grad():
                outputs = self.dino_model(**inputs)
            # outputs.logits shape: [1, num_boxes, num_queries]
            # æ¯ä¸ª claim å¯¹åº”ä¸€ä¸ª queryï¼Œå–å„ query çš„æœ€å¤§ box score
            logits = outputs.logits.sigmoid()  # [1, num_boxes, num_queries]
            if logits.dim() == 3 and logits.shape[2] >= len(claims):
                scores = [logits[0, :, q].max().item() for q in range(len(claims))]
            else:
                # fallback: æ‰€æœ‰ claims å…±äº«ä¸€ä¸ª score
                s = logits.max().item()
                scores = [s] * len(claims)
            return scores
        except:
            return [0.0] * len(claims)

    def _batch_verify_clip(self, claims, image):
        """æ‰¹é‡ CLIP éªŒè¯ï¼šä¸€å¼ å›¾çš„å¤šä¸ª claims ä½œä¸º text list"""
        if not self.clip_model:
            return [0.0] * len(claims)
        try:
            texts = [f"a photo of {c}" for c in claims]
            inputs = self.clip_processor(text=texts, images=image, return_tensors="pt", padding=True).to(self.clip_device)
            with torch.no_grad():
                outputs = self.clip_model(**inputs)
            # logits_per_image: [1, num_claims]
            scores = outputs.logits_per_image.softmax(dim=1)[0].tolist()
            return scores
        except:
            return [0.0] * len(claims)

    def verify_claims_batch(self, claims_by_image, preloaded_images=None):
        """
        æ‰¹é‡éªŒè¯ claimsï¼ŒæŒ‰å›¾ç‰‡åˆ†ç»„ï¼Œæ¯å¼ å›¾åªåŠ è½½ä¸€æ¬¡ã€‚
        claims_by_image: [(image_path, [claim1, claim2, ...]), ...]
        preloaded_images: å¯é€‰ dict {image_path: PIL.Image}ï¼Œé¿å…é‡å¤ç£ç›˜ IO
        è¿”å›: {(image_path, claim): verdict}
        """
        results = {}
        with ThreadPoolExecutor(max_workers=2) as executor:
            for img_path, claims in claims_by_image:
                if not claims:
                    continue
                # å°è¯•ä½¿ç”¨é¢„åŠ è½½å›¾ç‰‡
                image = None
                if preloaded_images and img_path in preloaded_images:
                    image = preloaded_images[img_path]
                else:
                    if not os.path.exists(img_path):
                        for c in claims:
                            results[(img_path, c)] = "uncertain"
                        continue
                    try:
                        image = Image.open(img_path).convert("RGB")
                    except:
                        for c in claims:
                            results[(img_path, c)] = "uncertain"
                        continue

                # å¹¶è¡Œæ‰§è¡Œ DINO å’Œ CLIPï¼ˆå®ƒä»¬åœ¨ä¸åŒè®¾å¤‡ä¸Šï¼‰
                dino_future = executor.submit(self._batch_verify_dino, claims, image)
                clip_future = executor.submit(self._batch_verify_clip, claims, image)
                dino_scores = dino_future.result()
                clip_scores = clip_future.result()

                for i, c in enumerate(claims):
                    score = max(dino_scores[i], clip_scores[i])
                    if score <= 0.0:
                        results[(img_path, c)] = "uncertain"
                    elif score > 0.5:
                        results[(img_path, c)] = "correct"
                    else:
                        results[(img_path, c)] = "incorrect"
        return results
