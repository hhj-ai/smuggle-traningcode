import torch
from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
import os
import argparse
from tqdm import tqdm
import json
import requests
import sys

# Configuration
DEFAULT_TRAINED_PATH = "./output/checkpoints/vlm_final"
BASE_MODEL_ID = "Qwen/Qwen3-VL-8B-Instruct" # åŸºåº§æ¨¡å‹ ID
TEST_IMAGE_DIR = "./data/test_images"
BENCHMARK_DIR = "./data/benchmarks"

# Auto-Download URLs for Benchmarks
POPE_URL = "https://huggingface.co/datasets/shiyue/POPE/resolve/main/coco_pope_random.json"
MMHAL_URL = "https://huggingface.co/datasets/Shengcao1006/MMHal-Bench/resolve/main/mmhal_bench.json"

POPE_DATA_PATH = os.path.join(BENCHMARK_DIR, "pope_coco_random.json")
MMHAL_DATA_PATH = os.path.join(BENCHMARK_DIR, "mmhal_bench.json")

def download_file(url, save_path):
    """Downloads a file from a URL if it doesn't exist."""
    if os.path.exists(save_path):
        return True
    
    print(f"â¬‡ï¸  Downloading benchmark data to {save_path}...")
    try:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        # ç®€å•çš„ä¸‹è½½é€»è¾‘ï¼Œå®é™…ç¯å¢ƒä¸­å»ºè®®ç”¨ setup_force_hf.sh æå‰ä¸‹å¥½
        response = requests.get(url, timeout=20) 
        if response.status_code != 200:
            print(f"âš ï¸  Direct download failed ({response.status_code}). Please use setup_force_hf.sh.")
            return False
        with open(save_path, "wb") as f:
            f.write(response.content)
        print(f"âœ… Saved.")
        return True
    except Exception as e:
        print(f"âŒ Failed to download: {e}")
        return False

def load_model(model_path):
    print(f"ğŸš€ Loading model from: {model_path} ...")
    try:
        processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            torch_dtype=torch.bfloat16, 
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="flash_attention_2" # H200 å¿…å¤‡åŠ é€Ÿ
        )
        return model, processor
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None, None

class POPEEvaluator:
    def __init__(self, data_path, image_dir):
        self.data_path = data_path
        self.image_dir = image_dir
        self.data = []
        
        if os.path.exists(data_path):
            with open(data_path, 'r') as f:
                self.data = [json.loads(line) for line in f]
        else:
            print(f"âš ï¸ POPE data missing at {data_path}")

    def evaluate(self, model, processor, prefix=""):
        if not self.data: return
        print(f"ğŸ“Š Running POPE Benchmark ({len(self.data)} samples)...")
        
        tp, tn, fp, fn = 0, 0, 0, 0
        missing_images = 0
        
        # Limit to 300 samples for speed during dev, remove slice for full run
        eval_data = self.data if len(self.data) < 500 else self.data[:300]
        
        for item in tqdm(eval_data):
            img_name = os.path.basename(item['image'])
            img_path = os.path.join(self.image_dir, img_name)
            
            # å°è¯•åœ¨ YFCC ç›®å½•æ‰¾æ‰¾ï¼ˆå¦‚æœç”¨æˆ·æŠŠ COCO å›¾æ”¾é‚£é‡Œäº†ï¼‰
            if not os.path.exists(img_path):
                 yfcc_path = os.path.join("./data/yfcc100m", img_name)
                 if os.path.exists(yfcc_path):
                     img_path = yfcc_path
                 else:
                    missing_images += 1
                    continue
            
            try:
                image = Image.open(img_path).convert("RGB")
            except:
                continue

            prompt = f"{item['question']} Answer Yes or No."
            
            messages = [
                {"role": "user", "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]}
            ]
            text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text_input], images=[image], return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=10)
                ans = processor.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).lower()
            
            pred_yes = "yes" in ans
            gt_yes = item['answer'].lower() == "yes"
            
            if gt_yes and pred_yes: tp += 1
            elif not gt_yes and not pred_yes: tn += 1
            elif not gt_yes and pred_yes: fp += 1
            elif gt_yes and not pred_yes: fn += 1
            
        if missing_images > 0:
            print(f"âš ï¸ Skipped {missing_images} samples (missing images).")

        precision = tp / (tp + fp + 1e-6)
        recall = tp / (tp + fn + 1e-6)
        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)
        acc = (tp + tn) / (len(eval_data) - missing_images + 1e-6)
        
        print(f"ğŸ“ˆ [POPE] Acc: {acc:.2f}, F1: {f1:.2f}, Precision: {precision:.2f}")
        
        # Save result summary
        res_file = f"{prefix}pope_score.json"
        with open(res_file, "w") as f:
            json.dump({"accuracy": acc, "f1": f1, "precision": precision}, f)
        print(f"âœ… POPE scores saved to {res_file}")

class MMHalEvaluator:
    def __init__(self, data_path, image_dir):
        self.data_path = data_path
        self.image_dir = image_dir
        self.data = []
        if os.path.exists(data_path):
            with open(data_path, 'r') as f:
                self.data = json.load(f)
        else:
            print(f"âš ï¸ MMHal data missing at {data_path}")

    def evaluate(self, model, processor, prefix=""):
        if not self.data: return
        print(f"ğŸ“Š Running MMHal-Bench ({len(self.data)} samples)...")
        
        results = []
        missing_images = 0
        
        # Limit for speed
        eval_data = self.data if len(self.data) < 100 else self.data[:50]
        
        for item in tqdm(eval_data):
            img_path = os.path.join(self.image_dir, item.get('image_id', 'unknown')) 
            if not os.path.exists(img_path): 
                missing_images += 1
                continue

            try:
                image = Image.open(img_path).convert("RGB")
            except:
                continue
            
            messages = [
                {"role": "user", "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": item['question']}
                ]}
            ]
            text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text_input], images=[image], return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=256)
                response = processor.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            results.append({
                "question_id": item.get('question_id'),
                "response": response,
                "gt": item.get('gt_answer', '')
            })
        
        out_file = f"{prefix}mmhal_results.json"
        print(f"âœ… MMHal results saved to '{out_file}'. (Skipped {missing_images} images)")
        with open(out_file, "w") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

def evaluate(model_path, image_dir, run_benchmarks, prefix=""):
    model, processor = load_model(model_path)
    if not model:
        return

    # 1. Standard Description Evaluation
    if not os.path.exists(image_dir):
        print(f"Test directory {image_dir} not found. Creating it.")
        os.makedirs(image_dir, exist_ok=True)
    
    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.png'))]
    if image_files:
        print(f"Starting standard evaluation on {len(image_files)} images...")
        results = []
        # Randomly sample 5 images if too many
        import random
        sample_files = image_files if len(image_files) <= 5 else random.sample(image_files, 5)
        
        for img_file in tqdm(sample_files):
            img_path = os.path.join(image_dir, img_file)
            try:
                image = Image.open(img_path).convert("RGB")
            except: continue
            
            messages = [
                {"role": "user", "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": "Describe this image in detail."}
                ]}
            ]
            text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text_input], images=[image], return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                generated_ids = model.generate(**inputs, max_new_tokens=128)
                generated_text = processor.decode(generated_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            print(f"\n[Image: {img_file}]\nDescription: {generated_text}")
            results.append({"image": img_file, "description": generated_text})
        
        out_txt = f"{prefix}evaluation_results.txt"
        with open(out_txt, "w") as f:
            for res in results:
                f.write(f"Image: {res['image']}\nDescription: {res['description']}\n\n")
        print(f"ğŸ“„ Standard results saved to {out_txt}")

    # 2. Run Benchmarks if requested
    if run_benchmarks:
        # Check files
        if not os.path.exists(POPE_DATA_PATH):
            print("âš ï¸ POPE JSON not found. Run setup_force_hf.sh first.")
        
        pope = POPEEvaluator(POPE_DATA_PATH, image_dir) 
        pope.evaluate(model, processor, prefix=prefix)
        
        mmhal = MMHalEvaluator(MMHAL_DATA_PATH, image_dir)
        mmhal.evaluate(model, processor, prefix=prefix)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, default=DEFAULT_TRAINED_PATH, help="Path to trained model")
    parser.add_argument("--image_dir", type=str, default=TEST_IMAGE_DIR, help="Directory containing test images")
    parser.add_argument("--benchmarks", action="store_true", help="Run POPE and MMHal benchmarks")
    # æ–°å¢åŸºåº§æµ‹è¯•å¼€å…³
    parser.add_argument("--baseline", action="store_true", help="Evaluate the Base Model (Qwen3-VL) instead of trained checkpoint")
    
    args = parser.parse_args()
    
    target_model = args.model_path
    output_prefix = ""
    
    if args.baseline:
        print("ğŸ“‰ MODE: Baseline Evaluation")
        print(f"   Target: {BASE_MODEL_ID}")
        target_model = BASE_MODEL_ID
        output_prefix = "baseline_"
    else:
        print("ğŸ“ˆ MODE: Trained Model Evaluation")
        print(f"   Target: {target_model}")
    
    evaluate(target_model, args.image_dir, args.benchmarks, prefix=output_prefix)
