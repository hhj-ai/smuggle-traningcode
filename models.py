import torch
import re
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoConfig

# ==============================================================================
# ğŸ”§ æ ¸å¿ƒä¿®æ”¹ï¼šåŠ¨æ€æ³¨å†Œ Qwen3 æ¶æ„åˆ«å
# ==============================================================================
def register_custom_architectures():
    """
    åœ¨å†…å­˜ä¸­å°† 'qwen3_vl' æ³¨å†Œä¸º 'Qwen2VL' çš„å­ç±»/åˆ«åã€‚
    è¿™æ ·æ— éœ€ä¿®æ”¹ config.json æ–‡ä»¶ï¼Œtransformers ä¹Ÿèƒ½æ­£ç¡®è¯†åˆ«æ¶æ„ã€‚
    """
    try:
        # å°è¯•å¯¼å…¥ Qwen2VL çš„é…ç½®å’Œæ¨¡å‹ç±»ï¼ˆéœ€è¦ transformers >= 4.45.0ï¼‰
        from transformers import Qwen2VLConfig, Qwen2VLForConditionalGeneration
        
        print("ğŸ› ï¸  æ­£åœ¨æ‰§è¡Œæ¶æ„æ³¨å†Œ: Mapping 'qwen3_vl' -> Qwen2VL classes...")
        
        # 1. æ³¨å†Œé…ç½®ç±»ï¼šå‘Šè¯‰ AutoConfig é‡åˆ° "qwen3_vl" æ—¶ä½¿ç”¨ Qwen2VLConfig
        AutoConfig.register("qwen3_vl", Qwen2VLConfig)
        
        # 2. æ³¨å†Œæ¨¡å‹ç±»ï¼šå‘Šè¯‰ AutoModel é‡åˆ°è¿™ä¸ªé…ç½®æ—¶åŠ è½½å“ªä¸ªæ¨¡å‹ç±»
        AutoModelForCausalLM.register(Qwen2VLConfig, Qwen2VLForConditionalGeneration)
        
        print("âœ…  æ¶æ„æ³¨å†ŒæˆåŠŸï¼ç°åœ¨å¯ä»¥ç›´æ¥åŠ è½½ Qwen3-VL äº†ã€‚")
        
    except ImportError:
        print("\nâš ï¸  [ä¸¥é‡è­¦å‘Š] ä½ çš„ transformers ç‰ˆæœ¬è¿‡ä½ï¼Œæ— æ³•å¯¼å…¥ Qwen2VL åŸºç±»ï¼")
        print("   è¿™ä¼šå¯¼è‡´ Qwen3-VL åŠ è½½å¤±è´¥ã€‚è¯·åŠ¡å¿…è¿è¡Œ: pip install --upgrade transformers\n")
    except Exception as e:
        print(f"âš ï¸  æ¶æ„æ³¨å†Œè¿‡ç¨‹ä¸­å‡ºç°éè‡´å‘½é”™è¯¯: {e}")

# åœ¨æ¨¡å—å¯¼å…¥æ—¶ç«‹å³æ‰§è¡Œæ³¨å†Œ
register_custom_architectures()
# ==============================================================================

class VerifierModel:
    """
    Wrapper for Verifier.
    Defaults to local path './models/DeepSeek-R1-Distill-Qwen-7B'.
    """
    def __init__(self, model_name="./models/DeepSeek-R1-Distill-Qwen-7B", device="cuda"):
        self.device = device
        
        # è·¯å¾„æ£€æŸ¥
        if not os.path.exists(model_name):
            print(f"âš ï¸ Warning: Local model path '{model_name}' not found. Fallback to HF ID.")
            if "models/" in model_name: 
                model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        
        print(f"Loading Verifier from: {model_name} ...")
        
        # DeepSeek R1 ä½¿ç”¨çš„æ˜¯æ ‡å‡†çš„ Llama/Qwen ç»“æ„ï¼Œé€šå¸¸ä¸éœ€è¦ç‰¹æ®Šæ³¨å†Œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map={"": device}, 
            trust_remote_code=True,
            attn_implementation="flash_attention_2"
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def verify_claims(self, description):
        prompt = f"Extract distinct, verifiable visual claims from the following description. Format as a bulleted list.\n\nDescription: {description}\n\nClaims:"
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.6
        )
        
        raw_response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        clean_text = re.sub(r'<think>.*?</think>', '', raw_response, flags=re.DOTALL).strip()
        
        claims = []
        for line in clean_text.split('\n'):
            cleaned = line.strip().lstrip('-').lstrip('*').strip()
            if len(cleaned) > 5:
                claims.append(cleaned)
                
        return claims, raw_response

    def compute_sequence_log_prob(self, prompt, completion):
        full_prompt = f"Extract distinct, verifiable visual claims from the following description. Format as a bulleted list.\n\nDescription: {prompt}\n\nClaims:"
        full_text = full_prompt + completion
        
        inputs = self.tokenizer(full_text, return_tensors="pt").to(self.device)
        labels = inputs.input_ids.clone()
        
        prompt_ids = self.tokenizer(full_prompt, return_tensors="pt").input_ids
        prompt_len = prompt_ids.shape[1]
        safe_len = min(prompt_len, labels.shape[1])
        labels[:, :safe_len] = -100
        
        outputs = self.model(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            labels=labels
        )
        
        valid_token_count = (labels != -100).sum().item()
        if valid_token_count == 0:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
            
        total_log_prob = -outputs.loss * valid_token_count
        return total_log_prob

class VLMModel:
    """
    Wrapper for VLM.
    Defaults to local path './models/Qwen3-VL-8B-Instruct'.
    """
    def __init__(self, model_name="./models/Qwen3-VL-8B-Instruct", device="cuda"):
        self.device = device
        
        if not os.path.exists(model_name):
            print(f"âš ï¸ Warning: Local model path '{model_name}' not found. Fallback to HF ID.")
            if "models/" in model_name:
                model_name = "Qwen/Qwen3-VL-8B-Instruct"

        print(f"Loading VLM from: {model_name} ...")
        
        try:
            # è¿™é‡Œçš„ Processor åŠ è½½é€šå¸¸ä¾èµ– qwen2_vl çš„å¤„ç†é€»è¾‘
            self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
            
            # ç”±äºæˆ‘ä»¬åœ¨æ–‡ä»¶å¤´éƒ¨åšäº† register_custom_architectures()
            # è¿™é‡Œ AutoModel åº”è¯¥èƒ½è‡ªåŠ¨è¯†åˆ« qwen3_vl å¹¶è°ƒç”¨ Qwen2VL ç±»
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                device_map={"": device},
                trust_remote_code=True,
                attn_implementation="flash_attention_2"
            )
        except Exception as e:
            print(f"âŒ VLM Load Error Details: {e}")
            raise RuntimeError(f"VLM Load Error: {e}")
            
        self.tokenizer = self.processor.tokenizer

    def generate_description_batch(self, image_inputs, num_generations=4):
        # Qwen2/3-VL çš„æ ‡å‡† Prompt æ ¼å¼
        text_prompts = ["Describe this image in detail."] * len(image_inputs)
        
        inputs = self.processor(
            text=text_prompts,
            images=image_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=True,
                temperature=1.0,
                num_return_sequences=num_generations
            )
        
        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)
        
        results = []
        for i in range(len(image_inputs)):
            start = i * num_generations
            results.append(generated_texts[start : start + num_generations])
            
        return results

    def compute_log_probs(self, input_ids, attention_mask, labels):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        valid_count = (labels != -100).sum().item()
        if valid_count == 0:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
            
        return -outputs.loss * valid_count
