#!/bin/bash
# --- 1. ç»å¯¹è·¯å¾„å®šä¹‰ ---
CODE_ROOT=$(cd "$(dirname "$0")"; pwd)
RES_ROOT=$(realpath "$CODE_ROOT/../aurora_resources")
PROD_ENV="/mnt/dolphinfs/ssd_pool/docker/user/hadoop-nlp-sh02/native_mm/zhangmanyuan/zhangquan/agent/xl/hhj-train/smuggle-traningcode/aurora_env"

# æ ¸å¿ƒèµ„æºè·¯å¾„
MODELS_DIR="$RES_ROOT/models"
DATA_DIR="$RES_ROOT/data"
OUTPUT_DIR="$RES_ROOT/output"

# --- 2. é²æ£’æ€§èµ„äº§æ£€æŸ¥ ---
echo "ðŸ” [Pre-flight] æ­£åœ¨æ£€æŸ¥æ ¸å¿ƒæ¨¡åž‹èµ„äº§..."
MISSING=0
for m in "Qwen3-VL-8B-Instruct" "DeepSeek-R1-Distill-Qwen-7B" "grounding-dino-base" "clip-vit-base-patch32" "minilm"; do
    if [ ! -d "$MODELS_DIR/$m" ]; then
        echo "âŒ ç¼ºå¤±æ¨¡åž‹: $MODELS_DIR/$m"
        MISSING=1
    fi
done

if [ $MISSING -eq 1 ]; then
    echo "ðŸš¨ èµ„äº§ä¸å…¨ï¼Œè¯·æ£€æŸ¥ ../aurora_resources/models ç›®å½•ï¼"
    # å¦‚æžœç¼ºå¤±ï¼Œå°è¯•å¯»æ‰¾ä½ çš„ SRC åŽŸå§‹å¤‡ä»½ï¼ˆä½ ä¹‹å‰è¯´å·²ç»å¤åˆ¶å¥½äº†ï¼Œå¦‚æžœæ²¡äº†è¿™é‡Œå¯ä»¥åŠ è‡ªåŠ¨ link é€»è¾‘ï¼‰
    exit 1
fi

# --- 3. è¿›ç¨‹æš´åŠ›æ¸…ç† ---
pkill -9 -f aurora_train.py 2>/dev/null
pkill -9 -f accelerate 2>/dev/null
sleep 2

# --- 4. çŽ¯å¢ƒå˜é‡æžè‡´è°ƒä¼˜ ---
export TRANSFORMERS_OFFLINE=1
export HF_DATASETS_OFFLINE=1
export HF_HOME="$RES_ROOT/hf_cache"
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NCCL_P2P_DISABLE=1
export NCCL_TIMEOUT=14400

source "$PROD_ENV/bin/activate"

# --- 5. å¯åŠ¨ ---
echo "ðŸ”¥ [GPU] å¯åŠ¨æžè‡´ç¨³å®šç‰ˆè®­ç»ƒ (èµ„äº§æ£€æŸ¥å·²é€šè¿‡)..."
LOG_NAME="train_robust_$(date +%Y%m%d_%H%M).log"

setsid accelerate launch \
    --multi_gpu --num_processes 8 --mixed_precision bf16 \
    aurora_train.py \
    --model_dir "$MODELS_DIR" \
    --data_dir "$DATA_DIR" \
    --minilm_path "$MODELS_DIR/minilm" \
    --output_dir "$OUTPUT_DIR" \
    --batch_size 16 \
    --attack_weight 5.0 > "$LOG_NAME" 2>&1 < /dev/null &

echo "ðŸš€ å·²å¯åŠ¨ï¼æ—¥å¿—: tail -f $LOG_NAME"
