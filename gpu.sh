#!/bin/bash
# --- ç»Ÿä¸€è·¯å¾„å®šä¹‰ ---
CODE_DIR=$(cd "$(dirname "$0")"; pwd)
RES_DIR=$(realpath "$CODE_DIR/../aurora_resources")
MODELS_DIR="$RES_DIR/models"
DATA_DIR="$RES_DIR/data"
OUTPUT_DIR="$RES_DIR/output"

# ç”Ÿäº§çŽ¯å¢ƒåŽŸå§‹çŽ¯å¢ƒè·¯å¾„ (ä½ çš„ä¿åº•è·¯å¾„)
PROD_ENV="/mnt/dolphinfs/ssd_pool/docker/user/hadoop-nlp-sh02/native_mm/zhangmanyuan/zhangquan/agent/xl/hhj-train/smuggle-traningcode/aurora_env"

echo "ðŸ“‚ [GPU] æ£€æŸ¥èµ„æºç›®å½•: $RES_DIR"

# 1. èµ„äº§å­˜åœ¨æ€§è‡ªæ£€
MISSING=0
for m in "Qwen3-VL-8B-Instruct" "DeepSeek-R1-Distill-Qwen-7B" "grounding-dino-base" "clip-vit-base-patch32" "minilm"; do
    if [ ! -d "$MODELS_DIR/$m" ]; then
        echo "âŒ ç¼ºå¤±æ¨¡åž‹: $MODELS_DIR/$m"
        MISSING=1
    fi
done

if [ $MISSING -eq 1 ]; then
    echo "ðŸš¨ èµ„äº§ç¼ºå¤±ï¼è¯·å…ˆåœ¨ CPU æœåŠ¡å™¨è¿è¡Œ cpu.sh æˆ–æ£€æŸ¥æŒ‚è½½ã€‚"
    exit 1
fi

# 2. è¿›ç¨‹æ¸…ç†
pkill -9 -f aurora_train.py 2>/dev/null
pkill -9 -f accelerate 2>/dev/null
sleep 2

# 3. çŽ¯å¢ƒå˜é‡
export TRANSFORMERS_OFFLINE=1
export HF_DATASETS_OFFLINE=1
export HF_HOME="$RES_DIR/hf_cache"
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export OMP_NUM_THREADS=1

# 4. æ¿€æ´»çŽ¯å¢ƒ (ä¼˜å…ˆä½¿ç”¨ç”Ÿäº§è·¯å¾„)
if [ -f "$PROD_ENV/bin/activate" ]; then
    source "$PROD_ENV/bin/activate"
else
    source "$RES_DIR/env/bin/activate"
fi

# 5. å¯åŠ¨
echo "ðŸ”¥ å¯åŠ¨ AURORA è®­ç»ƒ..."
LOG_NAME="train_final_$(date +%Y%m%d_%H%M).log"

setsid accelerate launch \
    --multi_gpu --num_processes 8 --mixed_precision bf16 \
    aurora_train.py \
    --model_dir "$MODELS_DIR" \
    --data_dir "$DATA_DIR/yfcc100m" \
    --minilm_path "$MODELS_DIR/minilm" \
    --output_dir "$OUTPUT_DIR" \
    --batch_size 16 > "$LOG_NAME" 2>&1 < /dev/null &

echo "ðŸš€ å·²åŽå°å¯åŠ¨ã€‚æ—¥å¿—: tail -f $LOG_NAME"
