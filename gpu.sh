#!/bin/bash
# ==============================================================================
# AURORA Installer (Sequential No-Deps Mode)
# ------------------------------------------------------------------------------
# æ ¸å¿ƒç­–ç•¥ï¼šé€ä¸ªå®‰è£…æ–‡ä»¶ï¼Œå®Œå…¨ç»•è¿‡ pip çš„ä¾èµ–è§£æå›¾ (Dependency Graph)ã€‚
# è§£å†³ error: resolution-too-deep
# ==============================================================================

BASE_DIR="./offline_packages"
WHEEL_DIR="$BASE_DIR/wheels"
INSTALL_ROOT="./aurora_env_root"
VENV_DIR="aurora_env"

echo "ğŸš€ [GPU Server] å¼€å§‹åºåˆ—åŒ–å®‰è£… (é˜²æ­¢ä¾èµ–æ­»é”)..."

# 1. ç¯å¢ƒå‡†å¤‡
[ ! -d "$INSTALL_ROOT" ] && mkdir -p $INSTALL_ROOT && tar -xzf "$BASE_DIR/python_runtime/python-3.10.tar.gz" -C $INSTALL_ROOT
if [ -d "$INSTALL_ROOT/python" ]; then EXE_PYTHON="$INSTALL_ROOT/python/bin/python3"; else EXE_PYTHON="$INSTALL_ROOT/bin/python3"; fi
[ ! -d "$VENV_DIR" ] && $EXE_PYTHON -m venv $VENV_DIR
source $VENV_DIR/bin/activate

# å®šä¹‰å¼ºåˆ¶å®‰è£…å‡½æ•°
force_install() {
    # --no-deps: åªè¦è§£å‹ï¼Œä¸è¦æ£€æŸ¥ä¾èµ–
    # --force-reinstall: ç¡®ä¿è¦†ç›–æ—§çš„é”™è¯¯ç‰ˆæœ¬
    python -m pip install "$1" --no-index --find-links=$WHEEL_DIR --no-deps --quiet
}

# 2. å…³é”®åº•å±‚åº“ (æ‰‹åŠ¨ä¼˜å…ˆå®‰è£…)
echo "ğŸ§± [1/5] å®‰è£…åº•å±‚æ„å»ºå·¥å…·..."
# å¿…é¡»å…ˆè£…è¿™äº›ï¼Œå¦åˆ™åé¢å¯èƒ½ä¼šæŠ¥é”™
for pkg in "numpy" "packaging" "wheel" "setuptools" "ninja"; do
    # æ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶
    PKG_FILE=$(find $WHEEL_DIR -name "$pkg*.whl" | head -n 1)
    if [ ! -z "$PKG_FILE" ]; then
        echo "   -> $pkg"
        force_install "$PKG_FILE"
    fi
done

# 3. æ ¸å¿ƒæ¡†æ¶
echo "ğŸ”¥ [2/5] å®‰è£… PyTorch & NVIDIA..."
# å®‰è£… Torch å…¨å®¶æ¡¶
find $WHEEL_DIR -name "torch*.whl" -o -name "nvidia*.whl" -o -name "triton*.whl" | while read whl; do
    force_install "$whl"
done

# 4. Flash Attention (æ˜¾å¼å®‰è£…)
echo "âš¡ [3/5] å®‰è£… Flash Attention..."
# ç§»é™¤å¯èƒ½å­˜åœ¨çš„æºç åŒ…
rm -f "$WHEEL_DIR/flash_attn"*.tar.gz
FA_WHEEL=$(find $WHEEL_DIR -name "flash_attn*.whl" | head -n 1)

if [ -f "$FA_WHEEL" ]; then
    echo "   -> Installing: $(basename $FA_WHEEL)"
    # è¿™é‡Œå¿…é¡»ç”¨ pip install æ–‡ä»¶è·¯å¾„ï¼Œä¸èƒ½ç”¨åŒ…å
    python -m pip install "$FA_WHEEL" --no-deps --no-index
else
    echo "âŒ ä¸¥é‡é”™è¯¯: æœªæ‰¾åˆ° Flash Attention Wheel!"
    exit 1
fi

# 5. æš´åŠ›å®‰è£…å‰©ä½™æ‰€æœ‰åŒ…
echo "ğŸ“š [4/5] åºåˆ—åŒ–å®‰è£…æ‰€æœ‰å‰©ä½™ä¾èµ– (è¿™å¯èƒ½éœ€è¦ä¸€åˆ†é’Ÿ)..."
# éå†ç›®å½•æ‰€æœ‰ whlï¼Œé€ä¸ªå®‰è£…ã€‚å¿½ç•¥é”™è¯¯ï¼ˆå› ä¸ºæœ‰çš„å·²ç»è£…è¿‡äº†ï¼‰
count=0
total=$(ls $WHEEL_DIR/*.whl | wc -l)
for whl in $WHEEL_DIR/*.whl; do
    count=$((count+1))
    # åªæ‰“å°è¿›åº¦æ¡ï¼Œä¸æ‰“å°è¯¦ç»†æ—¥å¿—
    echo -ne "   Processing $count/$total: $(basename $whl)\r"
    force_install "$whl"
done
echo ""

# 6. Transformers æºç 
echo "ğŸ¤— [5/5] å®‰è£… Transformers (Source)..."
if [ -f "$WHEEL_DIR/transformers-main.zip" ]; then
    # åŒæ ·åŠ ä¸Š --no-depsï¼Œé˜²æ­¢å®ƒå»è”ç½‘æ‰¾ tokenizers
    python -m pip install "$WHEEL_DIR/transformers-main.zip" --no-index --find-links=$WHEEL_DIR --no-deps
fi

# 7. æœ€ç»ˆè‡ªæ£€
echo "------------------------------------------------"
python <<EOF
import torch
print(f"âœ… PyTorch: {torch.__version__}")
try:
    import flash_attn
    print(f"âœ… FlashAttn: {flash_attn.__version__}")
except ImportError:
    print("âŒ FlashAttn Import Failed!")
try:
    import transformers
    print(f"âœ… Transformers: {transformers.__version__}")
except ImportError:
    print("âŒ Transformers Import Failed!")
try:
    from sentence_transformers import SentenceTransformer
    print(f"âœ… SentenceTransformers: OK (Scikit-learn detected)")
except Exception as e:
    print(f"âŒ ST Import Failed: {e}")
EOF
echo "------------------------------------------------"
echo "ğŸ‰ å®‰è£…å®Œæˆï¼æ‰€æœ‰åŒ…å·²å¼ºåˆ¶å°±ä½ã€‚"
echo "ğŸ‘‰ è¯·ä½¿ç”¨: python -m accelerate.commands.launch aurora_train.py"
